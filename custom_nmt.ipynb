{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF2F/ySQX/9J+KomMWYh8Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Calcifer777/learn-nlp/blob/main/custom_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "I2U-M7e_BiE-"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "riJVRmw6cvLH"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "ChhFmPFjiSug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "CAiPCzrnmQtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "test -d data || mkdir data \n",
        "wget -q https://raw.githubusercontent.com/L1aoXingyu/seq2seq-translation/master/data/eng-fra.txt -O data/eng-fra.txt"
      ],
      "metadata": {
        "id": "5yZifRuPmwVY"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "bo2C236WgW7T"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "cP9h1Ad-gLRa"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnFrDataset(Dataset):\n",
        "\n",
        "  def __init__(self, path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
        "      self.lines = [\n",
        "          [normalizeString(s.strip()) for s in l.split(\"\\t\")] \n",
        "          for l in fp.readlines()\n",
        "          if len(l.split(\"\\t\")) == 2\n",
        "      ]\n",
        "    self.lines = [l for l in self.lines if len(l[0]) < 50 and len(l[1]) < 50]\n",
        "    self.fields = [\"src\", \"tgt\"]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lines)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.lines[idx]\n"
      ],
      "metadata": {
        "id": "6Vh2ViIIejG2"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = EnFrDataset(path=\"data/eng-fra.txt\")"
      ],
      "metadata": {
        "id": "6i25uaI6fdqq"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[50003]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Di4rxBkhpO6",
        "outputId": "7de3da17-d5b5-4358-bddd-52e4b816bb79"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['she had a new dress made .', 'elle s est fait faire une nouvelle robe .']"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "HOPvpbrriViV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "-Jz8uCGbfi0x"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer \n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "E96_vO_tkJ1g"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8"
      ],
      "metadata": {
        "id": "FFwJ4vqJlkx2"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "_lB8ksjDibd4"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(samples):\n",
        "  samples = list(map(list, zip(*samples)))\n",
        "  tokenize = partial(tokenizer.batch_encode_plus, return_tensors=\"pt\", padding=True)\n",
        "  return dict(\n",
        "      src_tokens=tokenize(samples[0])[\"input_ids\"],\n",
        "      tgt_tokens=tokenize(samples[1])[\"input_ids\"]\n",
        "  )"
      ],
      "metadata": {
        "id": "M7Ik7EUkgpH9"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl = DataLoader(\n",
        "    dataset=ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "PyjWpakDimbw"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch = next(iter(dl))"
      ],
      "metadata": {
        "id": "ejEG2jU0jSau"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "OJBUuHZYn2L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "8YAxQwdHk0R-"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE = 128"
      ],
      "metadata": {
        "id": "svUNdL5QpNnL"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        hidden = self.init_hidden(input.shape[0]) if hidden is None else hidden\n",
        "        o, h = self.gru(self.embedding(input), hidden)\n",
        "        return {\"output\": o, \"hidden\": h}\n",
        "  \n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)"
      ],
      "metadata": {
        "id": "zqUAYPSFdHTj"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_size=tokenizer.vocab_size, hidden_size=HIDDEN_SIZE)"
      ],
      "metadata": {
        "id": "1QrtzU9pk-ca"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_out = encoder(\n",
        "    input=sample_batch[\"src_tokens\"], \n",
        "    # hidden=encoder.initHidden(batch_size=BATCH_SIZE)\n",
        ")\n",
        "print(f\"{enc_out['output'].shape = }\")\n",
        "print(f\"{enc_out['hidden'].shape = }\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03UNtCRilGcK",
        "outputId": "2b72198c-6db5-4158-b98e-7c0e9fba4717"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enc_out['output'].shape = torch.Size([8, 11, 128])\n",
            "enc_out['hidden'].shape = torch.Size([1, 8, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=1, batch_first=True)\n",
        "        self.attn_combine = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden, enc_output):\n",
        "        \n",
        "        # embeddings: batch, 1, H\n",
        "        embeddings = self.embedding(input)\n",
        "\n",
        "        # attn_scores: batch, 1, H\n",
        "        # attn_weights: batch, 1, T_src\n",
        "        attn_scores, attn_weights = self.mha(hidden.permute(1, 0, 2), enc_output, enc_output)\n",
        "\n",
        "        # Combine attn outputs with decoder input via a FF layer\n",
        "        # rnn_input: batch, 1, H\n",
        "        rnn_input = self.attn_combine(torch.cat([embeddings, attn_scores], dim=-1))\n",
        "        rnn_input = F.relu(rnn_input)\n",
        "\n",
        "        # Feed the rnn input and the previous hidden state to the RNN layer\n",
        "        # rnn_o: batch, 1, H\n",
        "        # rnn_h: 1, batch, H\n",
        "        rnn_o, rnn_h = self.gru(rnn_input, hidden)\n",
        "\n",
        "        # out: batch, 1, vocab_tgt_size\n",
        "        out = self.out(rnn_o)\n",
        "        out = F.log_softmax(out, dim=-1)\n",
        "        \n",
        "        return {\n",
        "            \"output\": out,\n",
        "            \"hidden\": rnn_h,\n",
        "            \"attention_weights\": attn_weights,\n",
        "        }"
      ],
      "metadata": {
        "id": "xh7JWsp8lU0Y"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(\n",
        "    input_size=tokenizer.vocab_size,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    output_size=tokenizer.vocab_size,\n",
        ")\n",
        "self = decoder"
      ],
      "metadata": {
        "id": "1Te0MYAeopDl"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_out = decoder(\n",
        "    input=sample_batch[\"tgt_tokens\"][:, :1],\n",
        "    hidden=torch.rand(1, BATCH_SIZE, HIDDEN_SIZE),\n",
        "    enc_output=enc_out['output'],\n",
        ")\n",
        "print(f\"{decoder_out['output'].shape = }\")\n",
        "print(f\"{decoder_out['hidden'].shape = }\")\n",
        "print(f\"{decoder_out['attention_weights'].shape = }\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEaoz4nxpS33",
        "outputId": "e5c60182-b94f-4292-ffb9-db7c5dd9def6"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_out['output'].shape = torch.Size([8, 1, 30522])\n",
            "decoder_out['hidden'].shape = torch.Size([1, 8, 128])\n",
            "decoder_out['attention_weights'].shape = torch.Size([8, 1, 11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, teacher_forcing_ratio=0.5, max_length=50, cls_token=100):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.encoder = Encoder(input_size=input_size, hidden_size=hidden_size)\n",
        "    self.decoder = Decoder(input_size=input_size, hidden_size=hidden_size, output_size=output_size, )\n",
        "\n",
        "    self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "    self.max_length = max_length\n",
        "    self.cls_token = cls_token\n",
        "\n",
        "  def forward(self, input_enc, input_dec):\n",
        "    enc_out = self.encoder(input_enc)\n",
        "\n",
        "    use_teacher_forcing = False # random.rand() < self.teacher_forcing_ratio\n",
        "\n",
        "    outputs = []\n",
        "    attention_weights = []\n",
        "    \n",
        "    batch_size = input_enc.shape[0]\n",
        "    dec_in = torch.ones((batch_size, 1)).long() * self.cls_token\n",
        "    dec_h = enc_out[\"hidden\"]\n",
        "    \n",
        "    target_length = input_dec.shape[1]\n",
        "    for idx in range(target_length):\n",
        "\n",
        "      dec_out = self.decoder(\n",
        "        input=dec_in,\n",
        "        hidden=dec_h,\n",
        "        enc_output=enc_out[\"output\"],\n",
        "      )\n",
        "      dec_h = dec_out[\"hidden\"]\n",
        "      outputs.append(dec_out[\"output\"])\n",
        "      attention_weights.append(dec_out[\"attention_weights\"])\n",
        "\n",
        "      if use_teacher_forcing is False:\n",
        "        # Use decoder prediction as next output\n",
        "        _, top_idx = dec_out[\"output\"].topk(1)\n",
        "        dec_in = top_idx.squeeze(1).detach()\n",
        "      elif use_teacher_forcing is True and (idx < target_length + 1):\n",
        "        # Use target values as next output\n",
        "        # Skip this step if last iteration\n",
        "        dec_in = input_dec[:, idx].unsqueeze(1)\n",
        "\n",
        "    return {\n",
        "        # batch, T_tgt, vocab_size_tgt\n",
        "        \"output\": torch.cat(outputs, dim=1),\n",
        "        # batch, T_src, T_tgt\n",
        "        \"attention_weights\": torch.cat(attention_weights, dim=1),\n",
        "    }\n",
        "    \n"
      ],
      "metadata": {
        "id": "AM70HtiOp3H0"
      },
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2Seq(\n",
        "     input_size=tokenizer.vocab_size,\n",
        "     hidden_size=HIDDEN_SIZE,\n",
        "     output_size=tokenizer.vocab_size,\n",
        ")\n",
        "self = seq2seq"
      ],
      "metadata": {
        "id": "uEm3ux-qwLdy"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_enc = sample_batch[\"src_tokens\"]\n",
        "input_dec = sample_batch[\"tgt_tokens\"]\n",
        "\n",
        "enc_out, enc_hidden = self.encoder(input_enc)\n",
        "\n",
        "seq2seq_out = seq2seq(\n",
        "    input_enc=input_enc,\n",
        "    input_dec=input_dec,\n",
        ")"
      ],
      "metadata": {
        "id": "mnlnnPgAwtdt"
      },
      "execution_count": 380,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_out[\"output\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRlaDvVZQ7hm",
        "outputId": "6fbfe5ca-ab13-482d-915a-276b7ec9dc78"
      },
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 17, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAFoz8apSJeS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}