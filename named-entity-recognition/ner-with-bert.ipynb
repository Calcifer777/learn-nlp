{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6586f9-2ec0-461c-9ffb-071f222cb13f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://stackoverflow.com/questions/69195950/problem-with-inputs-when-building-a-model-with-tfbertmodel-and-autotokenizer-fro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5dd0f9-e22f-4426-8338-67d68e2e4894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 13:55:23.282749: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-08 13:55:23.584253: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-08 13:55:23.584275: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-08 13:55:24.788303: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-08 13:55:24.788480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-08 13:55:24.788496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers as L\n",
    "import keras.models as M\n",
    "import keras.losses as LL\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_models as tfm\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, TFDistilBertForTokenClassification\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20cbe9f7-fd13-4799-be2d-7a40aeb7c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 13:55:28.289450: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-08 13:55:28.289560: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-08 13:55:28.289654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (calcifer-Inspiron-7370): /proc/driver/nvidia/version does not exist\n",
      "2023-01-08 13:55:28.290311: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_val, ds_test), info = tfds.load(\"conll2003\", with_info=True, split=[\"train\", \"dev\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "95cd4145-c603-4e2d-bfa5-eedff462ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_names = info.features[\"ner\"].names\n",
    "tag_ids = range(len(tag_names))\n",
    "label_ids_to_tags = dict(enumerate(tag_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e18c2b3-21c9-4762-81c5-afefa60a36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 13:55:28.932458: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chunks': array([ 0, 17, 11, 21, 22, 11, 12, 12, 12,  0,  3, 11, 12, 12, 21, 22, 22,\n",
       "        11, 12, 12, 12,  0,  0, 21, 11, 12, 12, 12, 11, 21, 22, 22, 22,  0]),\n",
       " 'ner': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'pos': array([ 0, 15, 28, 41, 39, 15, 18, 11, 21,  6, 30, 29, 16, 24, 41, 30, 39,\n",
       "        11, 10, 11, 21,  6,  0, 38, 11, 22, 24, 21, 44, 38, 35, 37, 40,  7]),\n",
       " 'tokens': array([b'\"', b'If', b'they', b\"'re\", b'saying', b'at', b'least', b'20',\n",
       "        b'percent', b',', b'then', b'their', b'internal', b'forecasts',\n",
       "        b'are', b'probably', b'saying', b'25', b'or', b'30', b'percent',\n",
       "        b',', b'\"', b'said', b'one', b'Sydney', b'media', b'analyst',\n",
       "        b'who', b'declined', b'to', b'be', b'named', b'.'], dtype=object)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ds_train.as_numpy_iterator().next()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c14df-18ef-417f-9260-ce97e1eedf28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b33b3fc5-29ee-42c8-8a98-a379747dac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILE = Path.cwd() / \"vocab.txt\"\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fdc082-552d-4c78-a6b7-4d0d90797980",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edb5aba1-9cce-4266-a1c7-f9b46a8dfc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[unused1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[unused2]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[unused3]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[unused4]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  idx\n",
       "0      [PAD]    0\n",
       "1  [unused1]    1\n",
       "2  [unused2]    2\n",
       "3  [unused3]    3\n",
       "4  [unused4]    4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    pd.DataFrame.from_dict(transformers_tokenizer.get_vocab(), orient=\"index\")\n",
    "    .reset_index()\n",
    "    .rename({\"index\": \"token\", 0: \"idx\"}, axis=1)\n",
    "    .sort_values(\"idx\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "736b8887-dc4c-4c51-a2ce-e12694840829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token\"].to_csv(path_or_buf=VOCAB_FILE, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34cf109-ec92-411d-bae5-7aab4513fa62",
   "metadata": {},
   "source": [
    "## Create the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1917f56a-9ac3-4418-8f39-aa34103fbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfm.nlp.layers.BertTokenizer(\n",
    "    vocab_file=VOCAB_FILE.as_posix(),\n",
    "    lower_case=False,\n",
    "    tokenize_with_offsets=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24bf9192-c0af-4b2f-bbb2-b40830b0e3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2023-01-08 13:55:30.421244: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([b'\"', b'If', b'they', b\"'re\", b'saying', b'at', b'least', b'20',\n",
       "       b'percent', b',', b'then', b'their', b'internal', b'forecasts',\n",
       "       b'are', b'probably', b'saying', b'25', b'or', b'30', b'percent',\n",
       "       b',', b'\"', b'said', b'one', b'Sydney', b'media', b'analyst',\n",
       "       b'who', b'declined', b'to', b'be', b'named', b'.'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = (\n",
    "    ds_train.map(lambda r: r[\"tokens\"])\n",
    "    .take(1)\n",
    "    .as_numpy_iterator().next()\n",
    ")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4d7dac95-1e37-4d15-8513-392e137195cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=array([  100,  1409,  1152,   112,  1231,  2157,  1120,  1655,  1406,\n",
      "        3029,   100,  1173,  1147,  4422, 24647,  1116,  1132,  1930,\n",
      "        2157,  1512,  1137,  1476,  3029,   100,   100,  1163,  1141,\n",
      "        3122,  2394, 14582,  1150,  5799,  1106,  1129,  1417,   119],\n",
      "      dtype=int32)\n",
      "x=<tf.RaggedTensor [[[100]], [[1409]], [[1152]], [[112], [1231]], [[2157]], [[1120]],\n",
      " [[1655]], [[1406]], [[3029]], [[100]], [[1173]], [[1147]], [[4422]],\n",
      " [[24647, 1116]], [[1132]], [[1930]], [[2157]], [[1512]], [[1137]],\n",
      " [[1476]], [[3029]], [[100]], [[100]], [[1163]], [[1141]], [[3122]],\n",
      " [[2394]], [[14582]], [[1150]], [[5799]], [[1106]], [[1129]], [[1417]],\n",
      " [[119]]]>\n"
     ]
    }
   ],
   "source": [
    "tokens, offsets_start, offsets_end = tokenizer(sample)\n",
    "np_tokens = tokens.merge_dims(0, 1).merge_dims(0, 1).numpy()\n",
    "pprint(np_tokens)\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9adcf0be-6ed3-48b0-ab37-435baadcb630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(36,), dtype=int32, numpy=\n",
       "array([  100,  1409,  1152,   112,  1231,  2157,  1120,  1655,  1406,\n",
       "        3029,   100,  1173,  1147,  4422, 24647,  1116,  1132,  1930,\n",
       "        2157,  1512,  1137,  1476,  3029,   100,   100,  1163,  1141,\n",
       "        3122,  2394, 14582,  1150,  5799,  1106,  1129,  1417,   119],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.merge_dims(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7bb9f051-8aa3-4e1d-a87e-dcd4cc1b3474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(34,), dtype=int64, numpy=\n",
       "array([1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.row_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1c6b2da7-6e23-43b2-9a5d-287e0b5f153d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(35,), dtype=int64, numpy=\n",
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.merge_dims(0, 1).row_lengths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7594c-c851-42f4-8a4d-5020d6fdb0fd",
   "metadata": {},
   "source": [
    "Check that the vocab porting was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c3a3ace2-3623-493e-b0c9-f6315b845055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'If',\n",
       " 'they',\n",
       " \"'\",\n",
       " 're',\n",
       " 'saying',\n",
       " 'at',\n",
       " 'least',\n",
       " '20',\n",
       " 'percent',\n",
       " '[UNK]',\n",
       " 'then',\n",
       " 'their',\n",
       " 'internal',\n",
       " 'forecast',\n",
       " '##s',\n",
       " 'are',\n",
       " 'probably',\n",
       " 'saying',\n",
       " '25',\n",
       " 'or',\n",
       " '30',\n",
       " 'percent',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'said',\n",
       " 'one',\n",
       " 'Sydney',\n",
       " 'media',\n",
       " 'analyst',\n",
       " 'who',\n",
       " 'declined',\n",
       " 'to',\n",
       " 'be',\n",
       " 'named',\n",
       " '.']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers_tokenizer.convert_ids_to_tokens(np_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405791c3-2cd1-40fa-9a3d-208ba99940b3",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e659e12c-0210-4cf6-b8ea-74fdc2ef732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(x):\n",
    "    print(f\"{x=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1ff7c83f-f18e-4a5d-bd54-d26868bcb2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(r):\n",
    "    tokens, offsets_start, offsets_end = tokenizer(r[\"tokens\"])\n",
    "    tokens_flat_0 = tokens.merge_dims(0, 1)\n",
    "    tokens_flat = tokens_flat_0.merge_dims(0, 1)\n",
    "    labels_aligned_0 = tf.repeat(r[\"ner\"], tokens.row_lengths())\n",
    "    labels_aligned = tf.repeat(labels_aligned_0, tokens_flat_0.row_lengths())\n",
    "    return tokens_flat, labels_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8847291b-e6d4-4741-99fd-1c39657df4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thailand     '            s            powerful     military     thinks       the          government   is           dish         ##ones       ##t          and          Prime        Minister     Ban          ##har        ##n         \n",
      "B-LOC        O            O            O            O            O            O            O            O            O            O            O            O            O            O            B-PER        B-PER        B-PER       \n",
      "\n",
      "\n",
      "L            ##OS         AN           ##GE         ##LE         ##S          70           60           .            53           ##8          1           \n",
      "B-ORG        B-ORG        I-ORG        I-ORG        I-ORG        I-ORG        O            O            O            O            O            O           \n",
      "\n",
      "\n",
      "Geelong      21           13           1            7            228          ##8          1940         117          .            9            54          \n",
      "B-ORG        O            O            O            O            O            O            O            O            O            O            O           \n",
      "\n",
      "\n",
      "Lau          ##ck         '            s            lawyer       vowed        he           would        appeal       against      the          court        '            s            decision     [UNK]        arguing      that        \n",
      "B-PER        B-PER        O            O            O            O            O            O            O            O            O            O            O            O            O            O            O            O           \n",
      "\n",
      "\n",
      "SE           ##AT         ##TL         ##E          66           63           .            512          8           \n",
      "B-ORG        B-ORG        B-ORG        B-ORG        O            O            O            O            O           \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 15:27:16.125914: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 18\n",
    "for tokens, labels in ds_train.map(tokenize).shuffle(10).take(5):\n",
    "    bert_words = transformers_tokenizer.convert_ids_to_tokens(tokens)\n",
    "    print(*[f\"{x:<12}\" for x in bert_words][:MAX_LENGTH])\n",
    "    print(*[f\"{label_ids_to_tags[x]:<12}\" for x in labels.numpy()][:MAX_LENGTH])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa43d0-366f-4011-bfce-6066f411969a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18425604-a59d-4954-bc9e-d4991a61ccc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e1824-823d-4973-8221-20a6619443c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df28a3-f34a-4260-ab40-4c0245032798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b40a7aa4-f607-42fc-9018-44f321d63b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ddba0-7d5b-492d-bf1f-a54ac1975a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba5bb1-4811-4e43-a290-7a78839f95a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa8ac7-060b-4fed-8479-27cd4d7093c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298e610-58c3-4493-b709-187a94d8ec95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bde7f-fa02-491f-8f20-539c25ea4886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7a93b-5682-4888-b54e-3ba913181f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa2dfe-9a0d-47b9-b9e6-fbfb97243a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5eaf8885-a7c5-4bce-aff5-65a31fa5390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_prep = (\n",
    "    ds_train.map(lambda r: (r[\"tokens\"], r[\"ner\"]))\n",
    "    .shuffle(buffer_size=BUFFER_SIZE)\n",
    "    .padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\n",
    ")\n",
    "ds_val_prep = (\n",
    "    ds_val.map(lambda r: (r[\"tokens\"], r[\"ner\"]))\n",
    "    .padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ca8fba4-eb29-42a4-94d2-2592ee12cdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 00:29:38.334500: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((64, 52), (64, 52))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ds_train_prep.as_numpy_iterator().next()\n",
    "sample[0].shape, sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d10e8ea-9ce6-4d12-98b9-ee9447e03f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 00:29:39.577616: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "sample = ds_train_prep.map(lambda tokens, ner: tokens).as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2d9ba-8efe-4613-8973-a042d9631c21",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8708a0b-498f-45b5-8f5a-8aa31e6e7900",
   "metadata": {},
   "source": [
    "### Setup the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f41e93f7-c69d-4287-9385-fd939e6ba887",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "beb69b9a-71b4-4748-9010-43626028ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5aea467b-5503-4d9b-9d4f-06dd3a36f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForTokenClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['dropout_59', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert = TFDistilBertForTokenClassification.from_pretrained(TRANSFORMER_MODEL_NAME, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54860794-5bcd-465b-91bf-4723356eabb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0afc5f3f-5166-4952-8d27-7ddb6819a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tensor):\n",
    "    arr = tensor.numpy().astype(str)\n",
    "    tokenized = transformers_tokenizer(arr.tolist(), return_tensors=\"tf\", is_split_into_words=True, padding=\"longest\")\n",
    "    return tokenized['input_ids'], tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3022a19-708d-4c69-bb71-2961b6772037",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = L.Input(shape=(None,), dtype=tf.string)\n",
    "input_ids, attention_mask = tf.py_function(tokenize, inp=[inputs], Tout=(tf.int32, tf.int32))\n",
    "input_ids = tf.ensure_shape(input_ids, (None, inputs.shape[0]))\n",
    "attention_mask = tf.ensure_shape(attention_mask, (None, inputs.shape[0]))\n",
    "x = distilbert(input_ids, attention_mask)[\"logits\"]\n",
    "x = L.Dense(units=len(tag_names), activation=\"softmax\")(x)\n",
    "model = M.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8147c4f-df9d-45b4-bc16-a4dd2b466b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_____________________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     Trainable  \n",
      "=============================================================================================================\n",
      " input_6 (InputLayer)           [(None, None)]       0           []                               Y          \n",
      "                                                                                                             \n",
      " tf.py_function_5 (TFOpLambda)  [None, None]         0           ['input_6[0][0]']                Y          \n",
      "                                                                                                             \n",
      " tf.ensure_shape_6 (TFOpLambda)  (None, None)        0           ['tf.py_function_5[0][0]']       Y          \n",
      "                                                                                                             \n",
      " tf.ensure_shape_7 (TFOpLambda)  (None, None)        0           ['tf.py_function_5[0][1]']       Y          \n",
      "                                                                                                             \n",
      " tf_distil_bert_for_token_class  TFTokenClassifierOu  65192450   ['tf.ensure_shape_6[0][0]',      Y          \n",
      " ification_2 (TFDistilBertForTo  tput(loss=None, log              'tf.ensure_shape_7[0][0]']                 \n",
      " kenClassification)             its=(None, None, 2)                                                          \n",
      "                                , hidden_states=Non                                                          \n",
      "                                e, attentions=None)                                                          \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| distilbert (TFDistilBertMainLa  multiple          65190912    []                               Y          |\n",
      "| yer)                                                                                                      |\n",
      "|                                                                                                           |\n",
      "| dropout_59 (Dropout)         multiple             0           []                               Y          |\n",
      "|                                                                                                           |\n",
      "| classifier (Dense)           multiple             1538        []                               Y          |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " dense_3 (Dense)                (None, None, 9)      27          ['tf_distil_bert_for_token_clas  Y          \n",
      "                                                                 sification_2[1][0]']                        \n",
      "                                                                                                             \n",
      "=============================================================================================================\n",
      "Total params: 65,192,477\n",
      "Trainable params: 65,192,477\n",
      "Non-trainable params: 0\n",
      "_____________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(expand_nested=True, show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "98b81768-7199-43ee-b512-3e9017571cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SK' 'Ceske' 'Budejovice' ... '' '' '']\n",
      " ['The' 'following' 'table' ... '' '' '']\n",
      " ['\"' 'But' 'the' ... '' '' '']\n",
      " ...\n",
      " ['HORSE' 'RACING' '-' ... '' '' '']\n",
      " ['Results' 'of' 'Major' ... '' '' '']\n",
      " ['Africa' ')' '66' ... '' '' '']]\n",
      "(64, 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 00:37:14.943847: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "arr, tags = ds_train_prep.as_numpy_iterator().next()\n",
    "arr = arr.astype(str)\n",
    "print(f\"{arr}\")\n",
    "print(f\"{arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "76f7c5e0-7c88-4c03-b7ab-9a23fdcfcb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 52)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e1c3da6-dfd7-4d27-b359-5f74f3e383e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = transformers_tokenizer(arr.tolist(), return_tensors=\"tf\", is_split_into_words=True, padding=True)\n",
    "tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "639c3079-d9ff-4e83-857f-3647f6ff34ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 725ms/step\n"
     ]
    }
   ],
   "source": [
    "output = model.predict(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "686a0349-2e7a-4e85-990a-ebe472435eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 66, 9])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.to_tensor().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e2315-ee4c-4ab0-9799-e524c46e75af",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "23737fec-a7d5-4262-87af-d3868fafb032",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(64, 52) and logits.shape=(64, 66, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparseCategoricalCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/losses.py:152\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 152\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m in_mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(y_pred)\n\u001b[1;32m    155\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(losses)\n",
      "File \u001b[0;32m~/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/losses.py:284\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    277\u001b[0m     y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[1;32m    278\u001b[0m         y_pred, y_true\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    281\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    283\u001b[0m )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/losses.py:2129\u001b[0m, in \u001b[0;36m_ragged_tensor_sparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;124;03m\"\"\"Implements support for handling RaggedTensors.\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \n\u001b[1;32m   2113\u001b[0m \u001b[38;5;124;03mExpected y_pred shape: (batch, sequence_len, n_classes) with sequence_len\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;124;03mthe sum of the individual loss values divided by 3.\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2123\u001b[0m fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   2124\u001b[0m     sparse_categorical_crossentropy,\n\u001b[1;32m   2125\u001b[0m     from_logits\u001b[38;5;241m=\u001b[39mfrom_logits,\n\u001b[1;32m   2126\u001b[0m     ignore_class\u001b[38;5;241m=\u001b[39mignore_class,\n\u001b[1;32m   2127\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   2128\u001b[0m )\n\u001b[0;32m-> 2129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ragged_tensor_apply_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_extra_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/losses.py:1570\u001b[0m, in \u001b[0;36m_ragged_tensor_apply_loss\u001b[0;34m(loss_fn, y_true, y_pred, y_pred_extra_dim)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_true, tf\u001b[38;5;241m.\u001b[39mRaggedTensor):\n\u001b[0;32m-> 1570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1572\u001b[0m lshape \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list()[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/losses.py:2098\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.metrics.sparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2050\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.losses.sparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     y_true, y_pred, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ignore_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m ):\n\u001b[1;32m   2056\u001b[0m     \u001b[38;5;124;03m\"\"\"Computes the sparse categorical crossentropy loss.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \n\u001b[1;32m   2058\u001b[0m \u001b[38;5;124;03m    Standalone usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;124;03m      Sparse categorical crossentropy loss value.\u001b[39;00m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_categorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/backend.py:5637\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   5633\u001b[0m         res \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msparse_softmax_cross_entropy_with_logits(\n\u001b[1;32m   5634\u001b[0m             labels\u001b[38;5;241m=\u001b[39mtarget, logits\u001b[38;5;241m=\u001b[39moutput\n\u001b[1;32m   5635\u001b[0m         )\n\u001b[1;32m   5636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5637\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_softmax_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\n\u001b[1;32m   5639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5642\u001b[0m     res_shape \u001b[38;5;241m=\u001b[39m cast(output_shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(64, 52) and logits.shape=(64, 66, 9)"
     ]
    }
   ],
   "source": [
    "LL.SparseCategoricalCrossentropy()(tags, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83b88813-e848-49d7-96cb-20cfe7f12e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "685841eb-5fbd-4158-9f21-93e87446eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=LL.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        keras.metrics.Accuracy(), \n",
    "        keras.metrics.TrueNegatives(), \n",
    "        keras.metrics.TruePositives(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b24bc3d2-bbd0-4bb5-956e-553eef92802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1028, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1122, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 691, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/metrics/metrics.py\", line 3571, in accuracy  **\n        y_true.shape.assert_is_compatible_with(y_pred.shape)\n\n    ValueError: Shapes (None, None) and (None, None, 9) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds_train_prep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val_prep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filea2p44m0u.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1028, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1122, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 691, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/keras/metrics/metrics.py\", line 3571, in accuracy  **\n        y_true.shape.assert_is_compatible_with(y_pred.shape)\n\n    ValueError: Shapes (None, None) and (None, None, 9) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    ds_train_prep,\n",
    "    validation_data=ds_val_prep,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac12a9-f0b3-4013-83b8-d6597d3d3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm.nlp.models.BertTokenClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
