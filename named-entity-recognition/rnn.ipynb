{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2078da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import losses as L\n",
    "from keras.layers import (\n",
    "    TimeDistributed,\n",
    "    Dense,\n",
    "    GRU,\n",
    "    Embedding,\n",
    "    TextVectorization,\n",
    "    Input,\n",
    "    StringLookup,\n",
    "    Bidirectional,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import CustomNonPaddingTokenLoss\n",
    "from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3215bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 12:13:40.342085: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-30 12:13:40.342413: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-30 12:13:40.342442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (calcifer-Inspiron-7370): /proc/driver/nvidia/version does not exist\n",
      "2022-12-30 12:13:40.343980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(ds_dev, ds_train, ds_test), info = tfds.load(\n",
    "    name=\"conll2003\",\n",
    "    split=[\"dev\", \"train\", \"test\",],\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aafa22e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='conll2003',\n",
       "    full_name='conll2003/conll2022/1.0.0',\n",
       "    description=\"\"\"\n",
       "    The shared task of CoNLL-2003 concerns language-independent named entity\n",
       "    recognition and concentrates on four types of named entities: persons,\n",
       "    locations, organizations and names of miscellaneous entities that do not belong\n",
       "    to the previous three groups.\n",
       "    \"\"\",\n",
       "    homepage='https://www.aclweb.org/anthology/W03-0419/',\n",
       "    data_path='/home/calcifer/tensorflow_datasets/conll2003/conll2022/1.0.0',\n",
       "    file_format=tfrecord,\n",
       "    download_size=959.94 KiB,\n",
       "    dataset_size=3.87 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'chunks': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=23)),\n",
       "        'ner': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=9)),\n",
       "        'pos': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=47)),\n",
       "        'tokens': Sequence(Text(shape=(), dtype=tf.string)),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'dev': <SplitInfo num_examples=3251, num_shards=1>,\n",
       "        'test': <SplitInfo num_examples=3454, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=14042, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n",
       "        title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n",
       "        author = \"Tjong Kim Sang, Erik F.  and\n",
       "          De Meulder, Fien\",\n",
       "        booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n",
       "        year = \"2003\",\n",
       "        url = \"https://www.aclweb.org/anthology/W03-0419\",\n",
       "        pages = \"142--147\",\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baeda1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQUENCE_LENGTH = 125\n",
    "EMBEDDING_DIM = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ce3a2f-3027-4773-a283-cf93b1cf5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    standardize=\"lower\",\n",
    "    split=None,\n",
    "    ragged=True,\n",
    "    # output_sequence_length=...,\n",
    ")\n",
    "\n",
    "vectorizer.adapt(ds_train.map(lambda r: r.get(\"tokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63848df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_names: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "label_names = info.features[\"ner\"].names\n",
    "print(\"label_names:\", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad0439cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_cld = (\n",
    "    ds_train\n",
    "    .map(lambda r: (vectorizer(r[\"tokens\"]), r[\"ner\"]+1))\n",
    "    .shuffle(buffer_size=100)\n",
    "    .padded_batch(batch_size=64, padded_shapes=([None], [None]))\n",
    ")\n",
    "ds_dev_cld = (\n",
    "    ds_dev\n",
    "    .map(lambda r: (vectorizer(r[\"tokens\"]), r[\"ner\"]+1))\n",
    "    .padded_batch(batch_size=64, padded_shapes=([None], [None]))\n",
    ")\n",
    "ds_test_cld = (\n",
    "    ds_test\n",
    "    .map(lambda r: (vectorizer(r[\"tokens\"]), r[\"ner\"]+1))\n",
    "    .padded_batch(batch_size=64, padded_shapes=([None], [None]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52c35b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape: (64, 41)\n",
      "Tags shape: (64, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 12:18:20.233140: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ds_dev_cld.take(1).as_numpy_iterator():\n",
    "    print(\"Tokens shape:\", x[0].shape)\n",
    "    print(\"Tags shape:\", x[1].shape)  # should be equal to tokens shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a571d44-07db-42df-9430-eb433b505732",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d943be25-c195-4518-b6a8-c959acde4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(ds, model):\n",
    "    prob_preds_test = model.predict(ds)\n",
    "    preds_test = [np.argmax(x, axis=-1) for x in prob_preds_test.to_list()]\n",
    "\n",
    "    labels_test = [r[1] for r in ds.unbatch().as_numpy_iterator()]\n",
    "\n",
    "    for idx in range(len(preds_test)):\n",
    "        assert preds_test[idx].shape == labels_test[idx].shape\n",
    "    \n",
    "    preds_idx_concat, labels_idx_concat = list(), list()\n",
    "\n",
    "    for (p, l) in zip(preds_dev, labels_dev):\n",
    "        mask = l > 0\n",
    "        preds_idx_concat += p[mask].tolist()\n",
    "        labels_idx_concat += l[mask].tolist()\n",
    "\n",
    "    preds_concat = [label_names[tag-1] for tag in preds_idx_concat]\n",
    "    labels_concat = [label_names[tag-1] for tag in labels_idx_concat]\n",
    "    \n",
    "    evaluate(labels_concat, preds_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02c3f6-0adb-4189-a374-5b6854752c14",
   "metadata": {},
   "source": [
    "## Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20fb7554-228f-4a98-bb10-00ac390c3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "inputs = Input(shape=(None,), name=\"tokens\")\n",
    "x = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    mask_zero=True,\n",
    ")(inputs)\n",
    "x = Bidirectional(layer=GRU(\n",
    "    units=128,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=True,\n",
    "))(x)\n",
    "outputs = TimeDistributed(layer=Dense(units=len(label_names)+1, activation=\"softmax\"), name=\"tags\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d85d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=CustomNonPaddingTokenLoss(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc7d869d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "220/220 [==============================] - 28s 105ms/step - loss: 1.6737 - accuracy: 0.8280 - val_loss: 1.6287 - val_accuracy: 0.8329\n",
      "Epoch 2/25\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.6287 - accuracy: 0.8334 - val_loss: 1.6285 - val_accuracy: 0.8332\n",
      "Epoch 3/25\n",
      "220/220 [==============================] - 29s 130ms/step - loss: 1.6286 - accuracy: 0.8336 - val_loss: 1.6285 - val_accuracy: 0.8334\n",
      "Epoch 4/25\n",
      "220/220 [==============================] - 31s 141ms/step - loss: 1.6282 - accuracy: 0.8337 - val_loss: 1.6278 - val_accuracy: 0.8337\n",
      "Epoch 5/25\n",
      "220/220 [==============================] - 31s 141ms/step - loss: 1.6245 - accuracy: 0.8377 - val_loss: 1.6149 - val_accuracy: 0.8459\n",
      "Epoch 6/25\n",
      "220/220 [==============================] - 27s 123ms/step - loss: 1.5993 - accuracy: 0.8621 - val_loss: 1.5984 - val_accuracy: 0.8621\n",
      "Epoch 7/25\n",
      "220/220 [==============================] - 28s 126ms/step - loss: 1.5951 - accuracy: 0.8654 - val_loss: 1.5974 - val_accuracy: 0.8625\n",
      "Epoch 8/25\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.5776 - accuracy: 0.8836 - val_loss: 1.5649 - val_accuracy: 0.8964\n",
      "Epoch 9/25\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.5436 - accuracy: 0.9179 - val_loss: 1.5439 - val_accuracy: 0.9172\n",
      "Epoch 10/25\n",
      "220/220 [==============================] - 25s 113ms/step - loss: 1.5130 - accuracy: 0.9493 - val_loss: 1.5253 - val_accuracy: 0.9360\n",
      "Epoch 11/25\n",
      "220/220 [==============================] - 27s 121ms/step - loss: 1.5045 - accuracy: 0.9571 - val_loss: 1.5239 - val_accuracy: 0.9369\n",
      "Epoch 12/25\n",
      "220/220 [==============================] - 27s 121ms/step - loss: 1.4983 - accuracy: 0.9635 - val_loss: 1.5144 - val_accuracy: 0.9470\n",
      "Epoch 13/25\n",
      "220/220 [==============================] - 26s 118ms/step - loss: 1.4909 - accuracy: 0.9708 - val_loss: 1.5112 - val_accuracy: 0.9503\n",
      "Epoch 14/25\n",
      "220/220 [==============================] - 26s 117ms/step - loss: 1.4872 - accuracy: 0.9745 - val_loss: 1.5093 - val_accuracy: 0.9521\n",
      "Epoch 15/25\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.4848 - accuracy: 0.9767 - val_loss: 1.5084 - val_accuracy: 0.9531\n",
      "Epoch 16/25\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.4833 - accuracy: 0.9783 - val_loss: 1.5082 - val_accuracy: 0.9529\n",
      "Epoch 17/25\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.4819 - accuracy: 0.9796 - val_loss: 1.5067 - val_accuracy: 0.9547\n",
      "Epoch 18/25\n",
      "220/220 [==============================] - 27s 122ms/step - loss: 1.4807 - accuracy: 0.9807 - val_loss: 1.5060 - val_accuracy: 0.9554\n",
      "Epoch 19/25\n",
      "220/220 [==============================] - 27s 122ms/step - loss: 1.4800 - accuracy: 0.9815 - val_loss: 1.5066 - val_accuracy: 0.9546\n",
      "Epoch 20/25\n",
      "220/220 [==============================] - 27s 122ms/step - loss: 1.4795 - accuracy: 0.9818 - val_loss: 1.5066 - val_accuracy: 0.9546\n",
      "Epoch 21/25\n",
      "220/220 [==============================] - 27s 123ms/step - loss: 1.4788 - accuracy: 0.9825 - val_loss: 1.5060 - val_accuracy: 0.9554\n",
      "Epoch 22/25\n",
      "220/220 [==============================] - 26s 118ms/step - loss: 1.4785 - accuracy: 0.9830 - val_loss: 1.5067 - val_accuracy: 0.9541\n",
      "Epoch 23/25\n",
      "220/220 [==============================] - 26s 120ms/step - loss: 1.4778 - accuracy: 0.9835 - val_loss: 1.5058 - val_accuracy: 0.9554\n",
      "Epoch 24/25\n",
      "220/220 [==============================] - 27s 122ms/step - loss: 1.4774 - accuracy: 0.9840 - val_loss: 1.5059 - val_accuracy: 0.9553\n",
      "Epoch 25/25\n",
      "220/220 [==============================] - 26s 120ms/step - loss: 1.4771 - accuracy: 0.9842 - val_loss: 1.5051 - val_accuracy: 0.9562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0ec16a590>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    ds_train_cld,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=ds_dev_cld,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd4180d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 2s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "prob_preds_test = model.predict(ds_test_cld)\n",
    "preds_test = [np.argmax(x, axis=-1) for x in prob_preds_test.to_list()]\n",
    "\n",
    "labels_test = [r[1] for r in ds_test_cld.unbatch().as_numpy_iterator()]\n",
    "\n",
    "for idx in range(len(preds_test)):\n",
    "    assert preds_test[idx].shape == labels_test[idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d53579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_idx_concat, labels_idx_concat = list(), list()\n",
    "\n",
    "for (p, l) in zip(preds_dev, labels_dev):\n",
    "    mask = l > 0\n",
    "    preds_idx_concat += p[mask].tolist()\n",
    "    labels_idx_concat += l[mask].tolist()\n",
    "    \n",
    "preds_concat = [label_names[tag-1] for tag in preds_idx_concat]\n",
    "labels_concat = [label_names[tag-1] for tag in labels_idx_concat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b431deb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51362 tokens with 5942 phrases; found: 5347 phrases; correct: 4531.\n",
      "accuracy:  76.22%; (non-O)\n",
      "accuracy:  95.44%; precision:  84.74%; recall:  76.25%; FB1:  80.27\n",
      "              LOC: precision:  89.67%; recall:  82.25%; FB1:  85.80  1685\n",
      "             MISC: precision:  86.82%; recall:  70.72%; FB1:  77.94  751\n",
      "              ORG: precision:  74.65%; recall:  63.24%; FB1:  68.47  1136\n",
      "              PER: precision:  85.63%; recall:  82.52%; FB1:  84.05  1775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84.73910604077052, 76.2537866038371, 80.27283196031534)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(labels_concat, preds_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8699f6-7dfa-478c-8473-e0413d0c6bfb",
   "metadata": {},
   "source": [
    "## Bi-LSTM with CRF\n",
    "\n",
    "Source: https://colab.research.google.com/drive/1kUUrn622sG9LeVz42XPpkLZEeoVW59Aq?usp=sharing#scrollTo=3b38225d9464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "82eba891-d3f7-47c2-8ddd-68b43c5c63d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tokens (InputLayer)         [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 256)        198144    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " tags (TimeDistributed)      (None, None, 16)          4112      \n",
      "                                                                 \n",
      " crf (CRF)                   [(None, None),            290       \n",
      "                              (None, None, 10),                  \n",
      "                              (None,),                           \n",
      "                              (10, 10)]                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,482,546\n",
      "Trainable params: 1,482,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "inputs = Input(shape=(None,), name=\"tokens\")\n",
    "x = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    mask_zero=True,\n",
    ")(inputs)\n",
    "x = Bidirectional(layer=GRU(\n",
    "    units=128,\n",
    "    dropout=0.2,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=True,\n",
    "))(x)\n",
    "x = TimeDistributed(layer=Dense(units=16, activation=\"relu\"), name=\"tags\")(x)\n",
    "decoded_sequence, potentials, sequence_length, chain_kernel = tfa.layers.CRF(units=len(label_names)+1, )(x)\n",
    "model = Model(inputs=inputs, outputs=[decoded_sequence, potentials, sequence_length, chain_kernel])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "03116df3-b985-413d-9b50-aa105d4f0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
    "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
    "        potentials, y, sequence_length, kernel\n",
    "    )\n",
    "    # likelihood to loss\n",
    "    flat_crf_loss = -1 * crf_likelihood\n",
    "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
    "\n",
    "    return crf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7798d5e4-9267-4c01-b0ba-9d39aba05dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
    "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
    "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb2e4a3a-7507-4c65-9601-448d148faecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 13:18:37.068467: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: StatefulPartitionedCall/cond_5/branch_executed/_1220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9.223627090454102\n",
      "Epoch 2, Loss: 2.494995355606079\n",
      "Epoch 3, Loss: 1.4105294942855835\n",
      "Epoch 4, Loss: 0.9632676839828491\n",
      "Epoch 5, Loss: 0.7145354747772217\n",
      "Epoch 6, Loss: 0.5659372210502625\n",
      "Epoch 7, Loss: 0.44172558188438416\n",
      "Epoch 8, Loss: 0.3533775806427002\n",
      "Epoch 9, Loss: 0.29467251896858215\n",
      "Epoch 10, Loss: 0.25024157762527466\n",
      "Epoch 11, Loss: 0.2025570273399353\n",
      "Epoch 12, Loss: 0.16012346744537354\n",
      "Epoch 13, Loss: 0.14195403456687927\n",
      "Epoch 14, Loss: 0.11544784903526306\n",
      "Epoch 15, Loss: 0.09686145186424255\n",
      "Epoch 16, Loss: 0.0824529379606247\n",
      "Epoch 17, Loss: 0.07007083296775818\n",
      "Epoch 18, Loss: 0.06280367076396942\n",
      "Epoch 19, Loss: 0.06254170835018158\n",
      "Epoch 20, Loss: 0.052692051976919174\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "\n",
    "    for x, y in ds_train_cld:\n",
    "        train_step(x, y)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, \" f\"Loss: {train_loss.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1c024c5a-83dc-4226-9607-0acd66e34005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: Stefanel Milan ( Italy ) 9 6 3 15\n",
      "True: ['B-ORG', 'I-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O']\n",
      "Pred: ['B-PER', 'B-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Tokens: Bowling : Wasim Akram 8.1-0-43-3 ( 9w , 1nb ) , Waqar Younis\n",
      "True: ['O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER']\n",
      "Pred: ['O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER']\n",
      "\n",
      "Tokens: Some said the central bank may have been concerned a weaker yen would lead to unfounded pessimism about Japan 's economy .\n",
      "True: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O']\n",
      "Pred: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O']\n",
      "\n",
      "Tokens: Zaragoza 15 2 8 5 18 23 14\n",
      "True: ['B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Pred: ['B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Tokens: SATURDAY , DECEMBER 7 SCHEDULE\n",
      "True: ['O', 'O', 'O', 'O', 'O']\n",
      "Pred: ['O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Tokens: MIAMI 14 5 .737 -\n",
      "True: ['B-ORG', 'O', 'O', 'O', 'O']\n",
      "Pred: ['B-ORG', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Tokens: Brendan Intindola\n",
      "True: ['B-PER', 'I-PER']\n",
      "Pred: ['B-PER', 'I-PER']\n",
      "\n",
      "Tokens: The government also wants British Airways to drop a clause in its agreement with USAir that bars it from competing on trans-Atlantic routes , and said both British Airways and American should be prepared to reduce services on the London to Dallas-Fort Worth route in the event that a new entrant wishes to enter .\n",
      "True: ['O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Pred: ['O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Tokens: Limoges ( France ) 9 3 6 12\n",
      "True: ['B-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O']\n",
      "Pred: ['B-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Tokens: But an open arbitrage to the U.S. and tight Italian supplies after Elf scooped up Med material over the last week , continued to underpin prices into next week .\n",
      "True: ['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-MISC', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Pred: ['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, r in enumerate(ds_test.take(10).as_numpy_iterator()):\n",
    "    print(\"Tokens:\", \" \".join(t.decode(\"utf-8\") for t in r[\"tokens\"]))\n",
    "    print(\"True:\", [label_names[tag-1] for tag in labels_test[idx] if tag > 0])\n",
    "    print(\"Pred:\", [label_names[tag-1] for tag in prob_preds_test[idx] if tag > 0])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d404ebf5-0fa2-460d-a8b0-e97f27e57a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(ds, model):\n",
    "    preds_test, *_ = model.predict(ds)\n",
    "    labels_test = [r[1] for r in ds.unbatch().as_numpy_iterator()]\n",
    "\n",
    "    for idx in range(len(labels_test)):\n",
    "        assert preds_test[idx].shape == labels_test[idx].shape, f\"error at id: {idx}\"\n",
    "    \n",
    "    preds_idx_concat, labels_idx_concat = list(), list()\n",
    "\n",
    "    for (p, l) in zip(preds_test, labels_test):\n",
    "        mask = l > 0\n",
    "        preds_idx_concat += p.numpy()[mask].tolist()\n",
    "        labels_idx_concat += l[mask].tolist()\n",
    "\n",
    "    preds_concat = [label_names[tag-1] for tag in preds_idx_concat]\n",
    "    labels_concat = [label_names[tag-1] for tag in labels_idx_concat]\n",
    "    \n",
    "    evaluate(labels_concat, preds_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe8548c2-0b3b-4e70-8992-966441783f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 2s 28ms/step\n",
      "processed 46435 tokens with 5648 phrases; found: 5219 phrases; correct: 3719.\n",
      "accuracy:  69.30%; (non-O)\n",
      "accuracy:  92.74%; precision:  71.26%; recall:  65.85%; FB1:  68.45\n",
      "              LOC: precision:  80.52%; recall:  76.32%; FB1:  78.36  1581\n",
      "             MISC: precision:  64.60%; recall:  60.83%; FB1:  62.66  661\n",
      "              ORG: precision:  61.99%; recall:  61.95%; FB1:  61.97  1660\n",
      "              PER: precision:  75.17%; recall:  61.22%; FB1:  67.48  1317\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(ds_test_cld, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442e2b2-5e24-4505-80f1-7ff545c0849c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
