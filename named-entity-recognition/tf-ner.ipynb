{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.layers import (\n",
    "    Embedding, \n",
    "    TextVectorization, \n",
    "    Bidirectional, \n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Input,\n",
    "    TextVectorization,\n",
    "    TimeDistributed,\n",
    "    GRU,\n",
    ")\n",
    "from conlleval import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 11:16:40.754209: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-29 11:16:40.754252: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-29 11:16:40.754279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (calcifer-Inspiron-7370): /proc/driver/nvidia/version does not exist\n",
      "2022-12-29 11:16:40.754572: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_test), info = tfds.load(\n",
    "  name=\"conll2003\",\n",
    "  split=[\"train\", \"test\"],\n",
    "  with_info=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='conll2003',\n",
      "    full_name='conll2003/conll2022/1.0.0',\n",
      "    description=\"\"\"\n",
      "    The shared task of CoNLL-2003 concerns language-independent named entity\n",
      "    recognition and concentrates on four types of named entities: persons,\n",
      "    locations, organizations and names of miscellaneous entities that do not belong\n",
      "    to the previous three groups.\n",
      "    \"\"\",\n",
      "    homepage='https://www.aclweb.org/anthology/W03-0419/',\n",
      "    data_path='/home/calcifer/tensorflow_datasets/conll2003/conll2022/1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=959.94 KiB,\n",
      "    dataset_size=3.87 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'chunks': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=23)),\n",
      "        'ner': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=9)),\n",
      "        'pos': Sequence(ClassLabel(shape=(), dtype=tf.int64, num_classes=47)),\n",
      "        'tokens': Sequence(Text(shape=(), dtype=tf.string)),\n",
      "    }),\n",
      "    supervised_keys=None,\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'dev': <SplitInfo num_examples=3251, num_shards=1>,\n",
      "        'test': <SplitInfo num_examples=3454, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=14042, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n",
      "        title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n",
      "        author = \"Tjong Kim Sang, Erik F.  and\n",
      "          De Meulder, Fien\",\n",
      "        booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n",
      "        year = \"2003\",\n",
      "        url = \"https://www.aclweb.org/anthology/W03-0419\",\n",
      "        pages = \"142--147\",\n",
      "    }\"\"\",\n",
      ")\n",
      "NER tags: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "print(info)\n",
    "print( \"NER tags:\", info.features[\"ner\"].feature.names)\n",
    "label_names = info.features[\"ner\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length in training set: 113\n"
     ]
    }
   ],
   "source": [
    "# Get max sequence length\n",
    "max_sequence_length = (\n",
    "  ds_train\n",
    "  .map(lambda r: len(r['tokens']))\n",
    "  .reduce(tf.constant(0, tf.int32), lambda t1, t2: tf.math.maximum(t1, t2) )\n",
    "  .numpy()\n",
    ")\n",
    "print(\"Max sequence length in training set:\", max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_cld = ds_train.filter(lambda r: len(r[\"tokens\"]) > 0)\n",
    "tokens = ds_train_cld.map(lambda r: r['tokens'])\n",
    "labels = ds_train_cld.map(lambda r: r[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "vectorizer = TextVectorization(\n",
    "  max_tokens=MAX_VOCAB_SIZE,\n",
    "  output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "  standardize=\"lower\",\n",
    "  output_mode=\"int\",\n",
    "  split=None,\n",
    ")\n",
    "\n",
    "vectorizer.adapt(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (\n",
    "  tokens\n",
    "  .map(vectorizer)\n",
    "  .filter(lambda r: len(r) > 0)\n",
    ")\n",
    "\n",
    "train_arr = np.vstack([x.numpy() for x in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pad_sequences(\n",
    "  sequences=[x.numpy()+1 for x in labels],\n",
    "  maxlen=MAX_SEQUENCE_LENGTH,\n",
    "  padding=\"post\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_DIM = len(info.features[\"ner\"].feature.names)+1\n",
    "\n",
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=np.int64)\n",
    "\n",
    "x = Embedding(\n",
    "  input_dim=MAX_VOCAB_SIZE, \n",
    "  output_dim=EMBEDDING_SIZE,\n",
    "  mask_zero=True,\n",
    ")(inputs)\n",
    "x = Bidirectional(\n",
    "        layer=GRU(\n",
    "        units=64, \n",
    "        return_sequences=True, \n",
    "        recurrent_dropout=0.2, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        dropout=0.2,\n",
    "    ),\n",
    ")(x)\n",
    "x = TimeDistributed(Dense(units=OUTPUT_DIM, activation=\"softmax\"))(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/nlp/ner_transformers/\n",
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=\"adam\", \n",
    "  loss=loss,\n",
    "  metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 128)         63744     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 100, 10)          1290      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,065,034\n",
      "Trainable params: 1,065,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 445ms/step\n",
      "(5, 100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\"\"\"\n",
    "output_shape: [records x sequence_length x  possible_tags]\n",
    "\n",
    "\"\"\"\n",
    "out = model.predict(train_arr[:5])\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_arr, train_labels)).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "220/220 [==============================] - 28s 107ms/step - loss: 1.6937 - accuracy: 0.8268\n",
      "Epoch 2/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.6287 - accuracy: 0.8330\n",
      "Epoch 3/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.6276 - accuracy: 0.8346\n",
      "Epoch 4/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.6130 - accuracy: 0.8483\n",
      "Epoch 5/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.5970 - accuracy: 0.8632\n",
      "Epoch 6/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.5826 - accuracy: 0.8794\n",
      "Epoch 7/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.5733 - accuracy: 0.8875\n",
      "Epoch 8/20\n",
      "220/220 [==============================] - 26s 120ms/step - loss: 1.5502 - accuracy: 0.9131\n",
      "Epoch 9/20\n",
      "220/220 [==============================] - 26s 120ms/step - loss: 1.5267 - accuracy: 0.9354\n",
      "Epoch 10/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.5217 - accuracy: 0.9399\n",
      "Epoch 11/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.5145 - accuracy: 0.9475\n",
      "Epoch 12/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.5076 - accuracy: 0.9543\n",
      "Epoch 13/20\n",
      "220/220 [==============================] - 26s 119ms/step - loss: 1.5044 - accuracy: 0.9570\n",
      "Epoch 14/20\n",
      "220/220 [==============================] - 26s 120ms/step - loss: 1.4945 - accuracy: 0.9677\n",
      "Epoch 15/20\n",
      "220/220 [==============================] - 29s 130ms/step - loss: 1.4888 - accuracy: 0.9732\n",
      "Epoch 16/20\n",
      "220/220 [==============================] - 31s 143ms/step - loss: 1.4857 - accuracy: 0.9762\n",
      "Epoch 17/20\n",
      "220/220 [==============================] - 31s 142ms/step - loss: 1.4840 - accuracy: 0.9778\n",
      "Epoch 18/20\n",
      "220/220 [==============================] - 37s 170ms/step - loss: 1.4825 - accuracy: 0.9793\n",
      "Epoch 19/20\n",
      "220/220 [==============================] - 37s 169ms/step - loss: 1.4815 - accuracy: 0.9802\n",
      "Epoch 20/20\n",
      "220/220 [==============================] - 36s 164ms/step - loss: 1.4806 - accuracy: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45654136a0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(\n",
    "  train_dataset,\n",
    "  epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 11s 25ms/step\n",
      "processed 203608 tokens with 23498 phrases; found: 22636 phrases; correct: 21295.\n",
      "accuracy:  90.33%; (non-O)\n",
      "accuracy:  98.20%; precision:  94.08%; recall:  90.62%; FB1:  92.32\n",
      "              LOC: precision:  95.44%; recall:  91.65%; FB1:  93.51  6857\n",
      "             MISC: precision:  92.03%; recall:  80.89%; FB1:  86.10  3022\n",
      "              ORG: precision:  92.20%; recall:  87.72%; FB1:  89.91  6013\n",
      "              PER: precision:  95.28%; recall:  97.36%; FB1:  96.31  6744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.07580844672204, 90.62473401991659, 92.31803008627043)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_arr\n",
    "y_padded = train_labels\n",
    "preds_padded = np.argmax(model.predict(x), axis=-1)\n",
    "y = list()\n",
    "preds = list()\n",
    "for p, l in zip(preds_padded, y_padded):\n",
    "    mask = l > 0\n",
    "    preds.append(p[mask])\n",
    "    y.append(l[mask])\n",
    "\n",
    "preds_concat = [label_names[tag-1] for tag in np.concatenate(preds)]\n",
    "y_concat = [label_names[tag-1] for tag in np.concatenate(y)]\n",
    "\n",
    "evaluate(y_concat, preds_concat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
