{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4725d949-c6b0-4d46-b9ba-7f981de492d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers as L\n",
    "from keras import models as M\n",
    "from keras import losses as LL\n",
    "from keras import callbacks as C\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from tensorflow_text import normalize_utf8\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52311c-3f7c-4035-a9c9-111cf913219f",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344dd867-1e85-46ed-8b56-a34a3f5a337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 358373 entries, 0 to 358372\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   eng     358373 non-null  object\n",
      " 1   ita     358373 non-null  object\n",
      " 2   author  358373 non-null  object\n",
      " 3   split   358373 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "path_to_file = pathlib.Path().home() / \"tensorflow_datasets\" / \"anki\" / \"ita-eng\" / \"ita.txt\"\n",
    "df = pd.read_csv(path_to_file, sep=\"\\t\", header=None)\n",
    "df.columns=[\"eng\", \"ita\", \"author\"]\n",
    "df[\"split\"] = np.where(np.random.uniform(size=(len(df),)) < 0.8, \"train\", \"val\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb1d9af-6156-41a2-a696-2e41e47769c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>ita</th>\n",
       "      <th>author</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Ciao!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Ciao.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corri!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corra!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Correte!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eng       ita                                             author  split\n",
       "0   Hi.     Ciao!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  train\n",
       "1   Hi.     Ciao.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  train\n",
       "2  Run!    Corri!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  train\n",
       "3  Run!    Corra!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  train\n",
       "4  Run!  Correte!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fdafc7-fc68-4d76-999e-645ff3b00324",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f88ba2-3adb-4224-b4b5-3f23d8d1cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d67532-c458-44bf-aed5-27ea5f04f203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 00:52:43.959446: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-06 00:52:43.959525: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-06 00:52:43.959591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (calcifer-Inspiron-7370): /proc/driver/nvidia/version does not exist\n",
      "2023-01-06 00:52:43.960406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((\n",
    "        df.query(\"split == 'train'\")[\"ita\"].values,\n",
    "        df.query(\"split == 'train'\")[\"eng\"].values,\n",
    "    ))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((\n",
    "        df.query(\"split == 'val'\")[\"ita\"].values,\n",
    "        df.query(\"split == 'val'\")[\"eng\"].values,\n",
    "    ))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe6486a-c367-429a-9748-3051e301aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def clean_string(s):\n",
    "    # Split accented characters\n",
    "    text = normalize_utf8(s, \"NFKD\")\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,;]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,;]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "    # Add START and END tokens\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b639ae9-7719-4292-bf7c-d4ec6cea6816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "vectorizer_src = L.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    standardize=clean_string,\n",
    "    ragged=True,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "vectorizer_src.adapt(train_raw.map(lambda src, dst: src))\n",
    "\n",
    "vectorizer_dst = L.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    standardize=clean_string,\n",
    "    ragged=True,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "vectorizer_dst.adapt(train_raw.map(lambda src, dst: dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d69425b3-8c1c-4209-a006-2bc2f8e3ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(src, dst):\n",
    "    src_enc = vectorizer_src(src).to_tensor()\n",
    "    dst_enc = vectorizer_dst(dst)\n",
    "    dst_in = dst_enc[:, :-1].to_tensor()\n",
    "    dst_out = dst_enc[:, 1:].to_tensor()\n",
    "    return (src_enc, dst_in), dst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a24daee-5103-49d8-8d41-8f99ccf349a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = train_raw.map(prep_dataset, tf.data.AUTOTUNE)\n",
    "ds_val = val_raw.map(prep_dataset, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebb4b43-e387-40bc-be90-0111ad02988b",
   "metadata": {},
   "source": [
    "## Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45dc30f9-0fca-40cf-a995-6e16f4bebef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "RNN_UNITS = 32\n",
    "ATTENTION_UNITS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4212642e-d2c5-45d0-bc01-7483daff91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_kwargs = dict(\n",
    "    dropout=0.2, \n",
    "    recurrent_dropout=0.2, \n",
    "    recurrent_initializer=\"glorot_uniform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0904b66-6659-483e-8da4-39020b1bf0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "        for name, new_dim in parsed.items():\n",
    "            old_dim = self.shapes.get(name, None)\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"    found: {new_dim}\\n\"\n",
    "                                 f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca17a6f9-fab8-4161-9d2a-3a827fc550da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(M.Model):\n",
    "    \n",
    "    def __init__(self, rnn_units: int, vocab_size: int, embedding_dim: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = L.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)\n",
    "        self.rnn = L.Bidirectional(\n",
    "            layer=L.GRU(units=rnn_units, return_sequences=True, **rnn_kwargs),\n",
    "            merge_mode=\"sum\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        outputs = self.rnn(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9be83aec-f7da-4604-ad82-ebde2771b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(L.Layer):\n",
    "    \n",
    "    def __init__(self, units: int):\n",
    "        super().__init__()\n",
    "        self.w1 = L.Dense(units=units, use_bias=False)\n",
    "        self.w2 = L.Dense(units=units, use_bias=False)\n",
    "        self.v = L.Dense(1, use_bias=False)\n",
    "\n",
    "    def call(self, query, keys):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, \"batch t_enc rnn_units\")\n",
    "        shape_checker(keys, \"batch t_dec emb_dec\")\n",
    "        # add nexaxis for broadcasting addition along the time dimensions\n",
    "        scores_query = tf.expand_dims(self.w1(query), axis=2)\n",
    "        scores_keys = tf.expand_dims(self.w2(keys), axis=1)\n",
    "        scores = self.v(tf.tanh(tf.add(scores_query, scores_keys)))\n",
    "        scores = tf.squeeze(scores, axis=-1)\n",
    "        shape_checker(scores, \"batch t_enc t_dec\")\n",
    "        attention_scores = tf.math.softmax(scores, axis=1)  # for each t_enc, unit sum\n",
    "        context = tf.matmul(attention_scores, keys)\n",
    "        return context, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3c613598-68cd-4cc2-a54d-ebbd8baad76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(M.Model):\n",
    "\n",
    "    def __init__(self, rnn_units: int, vocab_size: int, embedding_dim: int, attention_units: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = L.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)\n",
    "        self.rnn = L.GRU(units=rnn_units, return_state=True, return_sequences=True, **rnn_kwargs)\n",
    "        self.attention = AdditiveAttention(units=attention_units)\n",
    "        self.out = L.Dense(units=vocab_size)\n",
    "        self.last_attention_scores = None\n",
    "    \n",
    "    def call(self, inputs, enc_hidden_states, state=None, return_state=False):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(inputs, \"batch t_dec\")\n",
    "        shape_checker(enc_hidden_states, \"batch t_enc rnn_units\")\n",
    "        #\n",
    "        emb = self.embedding(inputs)\n",
    "        shape_checker(emb, \"batch t_dec emb\")\n",
    "        #\n",
    "        rnn_out, rnn_state = self.rnn(emb, initial_state=state)\n",
    "        shape_checker(rnn_out, \"batch t_dec rnn_units\")\n",
    "        #\n",
    "        context, attention_scores = self.attention(rnn_out, enc_hidden_states)\n",
    "        self.last_attention_scores = attention_scores\n",
    "        shape_checker(context, \"batch t_dec rnn_units\")\n",
    "        logits = self.out(context)\n",
    "        shape_checker(logits, 'batch t_dec vocab_size')\n",
    "        if return_state:\n",
    "            return logits, state\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9f331364-d963-4fd2-a9be-87befd7b1435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape_checker = ShapeChecker()\n",
    "enc_inputs, dec_inputs = ds_train.take(1).as_numpy_iterator().next()[0]\n",
    "shape_checker(enc_inputs, \"batch t_enc\")\n",
    "\n",
    "encoder = Encoder(rnn_units=RNN_UNITS, embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE)\n",
    "enc_outputs = encoder(enc_inputs)\n",
    "shape_checker(enc_outputs, \"batch t_enc rnn_units\")\n",
    "\n",
    "decoder = Decoder(rnn_units=RNN_UNITS, embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, attention_units=16)\n",
    "logits = decoder(dec_inputs[:, :3], enc_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f8f4a43b-c11e-44d4-8ae5-36cb9407b327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 3, 30000])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "fc705add-dfe4-4345-b5d5-aa5c42c12883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Translator(M.Model):\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         rnn_units: int = RNN_UNITS, \n",
    "#         vocab_size: int = VOCAB_SIZE, \n",
    "#         embedding_dim: int = EMBEDDING_DIM, \n",
    "#         attention_units: int = ATTENTION_UNITS,\n",
    "#     ):\n",
    "#         super(Translator, self).__init__()\n",
    "#         self.encoder = Encoder(rnn_units=rnn_units, embedding_dim=embedding_dim, vocab_size=vocab_size)\n",
    "#         self.decoder = Decoder(rnn_units=rnn_units, embedding_dim=embedding_dim, vocab_size=vocab_size, attention_units=attention_units)\n",
    "#     \n",
    "#     def call(self, inputs):\n",
    "#         inputs_enc, inputs_dec = inputs\n",
    "#         enc_hidden_states = self.encoder(inputs_enc)\n",
    "#         logits = self.decoder(inputs_dec, enc_hidden_states)\n",
    "#         return logits\n",
    "\n",
    "# Translator()((enc_inputs, dec_inputs))\n",
    "# print()\n",
    "\n",
    "# translator = Translator()\n",
    "# translator((enc_inputs, dec_inputs))\n",
    "# translator.summary(expand_nested=True)\n",
    "\n",
    "# translator.compile(\n",
    "#     optimizer=\"adam\",\n",
    "#     loss=masked_loss,\n",
    "#     metrics=[masked_accuracy, masked_loss],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "79920745-becc-479d-a9b8-2a6740cb3a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_36 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_67 (Encoder)           (None, None, 32)     3871104     ['input_35[0][0]']               \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| embedding_131 (Embedding)    multiple             3840000     []                               |\n",
      "|                                                                                                |\n",
      "| bidirectional_67 (Bidirectiona  multiple          31104       []                               |\n",
      "| l)                                                                                             |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " decoder_64 (Decoder)           (None, None, 30000)  4846592     ['input_36[0][0]',               \n",
      "                                                                  'encoder_67[0][0]']             \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| embedding_132 (Embedding)    multiple             3840000     []                               |\n",
      "|                                                                                                |\n",
      "| gru_132 (GRU)                multiple             15552       []                               |\n",
      "|                                                                                                |\n",
      "| additive_attention_64 (Additiv  multiple          1040        []                               |\n",
      "| eAttention)                                                                                    |\n",
      "|                                                                                                |\n",
      "| dense_259 (Dense)            multiple             990000      []                               |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "==================================================================================================\n",
      "Total params: 8,717,696\n",
      "Trainable params: 8,717,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs_enc = L.Input(shape=(None,))\n",
    "inputs_dec = L.Input(shape=(None,))\n",
    "encoder = Encoder(rnn_units=RNN_UNITS, embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE)\n",
    "decoder = Decoder(rnn_units=RNN_UNITS, embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, attention_units=ATTENTION_UNITS)\n",
    "enc_hidden_states = encoder(inputs_enc)\n",
    "logits = decoder(inputs_dec, enc_hidden_states)\n",
    "translator = M.Model(inputs=(inputs_enc, inputs_dec), outputs=logits)\n",
    "\n",
    "translator.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b425076-4c8c-489f-bab1-611a3b773393",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7010a676-238e-468e-8a93-dfaeebace7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    loss_fn = LL.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "    avg_loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2c5e581d-34f2-483e-843c-bf2ca2496a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    accuracy = tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e0130307-7c8e-4c07-bfb8-f1107e33526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_loss=9.490847, expected_accuracy=7.554011179936546e-05\n"
     ]
    }
   ],
   "source": [
    "vocab_size = float(vectorizer_dst.vocabulary_size())\n",
    "expected_loss = tf.math.log(vocab_size).numpy()\n",
    "expected_accuracy = 1.0 / vocab_size\n",
    "print(f\"{expected_loss=}, {expected_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0af7aef6-e1ad-4f43-a7e9-e5eb78274e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=masked_loss,\n",
    "    metrics=[masked_accuracy, masked_loss],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a896c39c-bb6e-4c24-aaed-46dfcf26d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 4s 111ms/step - loss: 10.3087 - masked_accuracy: 0.0000e+00 - masked_loss: 10.3087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 10.308748245239258,\n",
       " 'masked_accuracy': 0.0,\n",
       " 'masked_loss': 10.308748245239258}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.evaluate(ds_val, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa2f35-b834-4c02-ae8c-da6832bfc472",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "db864941-937b-4010-89f0-5c12b5e50318",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6d9b9ce9-9998-4ddb-b069-c57f9e7d9c25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 34s 252ms/step - loss: 7.2516 - masked_accuracy: 0.1935 - masked_loss: 7.2516 - val_loss: 4.7960 - val_masked_accuracy: 0.1929 - val_masked_loss: 4.7960\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 4.3220 - masked_accuracy: 0.2055 - masked_loss: 4.3220 - val_loss: 4.4696 - val_masked_accuracy: 0.1930 - val_masked_loss: 4.4696\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 4.3222 - masked_accuracy: 0.1983 - masked_loss: 4.3222 - val_loss: 4.3403 - val_masked_accuracy: 0.1942 - val_masked_loss: 4.3403\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 4.3322 - masked_accuracy: 0.1901 - masked_loss: 4.3322 - val_loss: 4.2431 - val_masked_accuracy: 0.1942 - val_masked_loss: 4.2431\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 36s 357ms/step - loss: 4.3331 - masked_accuracy: 0.1848 - masked_loss: 4.3331 - val_loss: 4.2303 - val_masked_accuracy: 0.1939 - val_masked_loss: 4.2303\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 4.3385 - masked_accuracy: 0.1810 - masked_loss: 4.3385 - val_loss: 4.1777 - val_masked_accuracy: 0.1935 - val_masked_loss: 4.1777\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 4.3555 - masked_accuracy: 0.1754 - masked_loss: 4.3555 - val_loss: 4.2280 - val_masked_accuracy: 0.1981 - val_masked_loss: 4.2280\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 4.3682 - masked_accuracy: 0.1754 - masked_loss: 4.3682 - val_loss: 4.2053 - val_masked_accuracy: 0.2063 - val_masked_loss: 4.2053\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 4.4155 - masked_accuracy: 0.1779 - masked_loss: 4.4155 - val_loss: 4.1713 - val_masked_accuracy: 0.2590 - val_masked_loss: 4.1713\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 4.4280 - masked_accuracy: 0.1765 - masked_loss: 4.4280 - val_loss: 4.2153 - val_masked_accuracy: 0.2415 - val_masked_loss: 4.2153\n"
     ]
    }
   ],
   "source": [
    "history = translator.fit(\n",
    "    ds_train.repeat(), \n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=ds_val,\n",
    "    validation_steps=20,\n",
    "    callbacks=[C.EarlyStopping(patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "90963496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe0cf792860>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4e750649",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'masked_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [263]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmasked_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_masked_acc\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylim([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m(plt\u001b[38;5;241m.\u001b[39mylim())])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'masked_acc'"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bc268-33c8-4a6e-80bc-a84e14f8e63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741ef53-ddf6-493f-8155-7e0ca342bd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8147c4-d7dc-415d-a882-72466c9b2672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f7f4c-6cfa-4bc1-a71d-7f96a4875feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6906169-2d8d-41a5-b6a1-cdaa301bbf49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2e99c-bde3-4e81-aadb-7ba95d329800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b5455-32f2-45d0-a2e5-bec024fd61e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda1ba8-6424-4382-a371-1927f55bd5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f5628-71b8-400c-b327-7deded886c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b5f5a-1e21-4119-a57b-de58e11037b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189adde4-a0cd-4aa2-94bb-83224d4713f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f311587-f4e9-473b-bf99-671403d36bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d422c05-4c95-4141-a6a2-9c35a73e9608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c583f2-b2ff-4e2d-80ff-7463dcf8d5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47de614-d98d-440f-94e8-a7fc6d7e49a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089be299-3f15-4d9a-8bf1-001229c63ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391e85d-42c1-42a3-bc9d-867e64b08506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81083ec-205b-479f-8514-55bb77844527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54846fac-46af-4998-84e5-9ea2dc4a24a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1d8c101f-f653-4d83-824c-dcd1cb66b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 8)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "28c04cd2-9796-4863-99d4-94bf6f155bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 8, 32])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "eb09cf6f-35ce-497a-9dc4-407609503e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_inputs[:, :3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "aff807b5-fb13-41fb-8ffa-a4826b951af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 3, 128])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_emb = decoder.embedding(dec_inputs[:, :3])\n",
    "decoder_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "99a81390-e029-4e6e-8874-1f7732ebb4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 32)\n",
      "(64, 32)\n"
     ]
    }
   ],
   "source": [
    "dec_rnn_out, state = decoder.rnn(decoder_emb)\n",
    "print(dec_rnn_out.shape)\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "c125e403-6518-4140-8f4b-25929acd5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = AdditiveAttention(units=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "2ab750ca-ac98-4020-8551-44b672a2ab27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1, 8, 16])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_keys = tf.expand_dims(attention.w1(enc_outputs), axis=1)\n",
    "scores_keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "2408db45-574c-41f4-b0f9-f1b876d2f27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 3, 1, 16])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_values = tf.expand_dims(attention.w2(dec_rnn_out), axis=2)\n",
    "scores_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0e38d6d8-46b4-4e2e-9db0-b438f7c7a981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 3, 8])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = attention.v(tf.tanh(tf.add(scores_keys, scores_values)))\n",
    "scores = tf.squeeze(scores)  # drop last dimension\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b41b3ad0-d854-48b2-97e3-cbb77cf0f708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 3, 8])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = tf.math.softmax(scores, axis=-1)\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "84cb71e1-d408-4c26-b62c-88987c6f7b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 3, 32])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(attention_scores, enc_outputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3993c02-8e99-435c-9205-3ecefa651325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
