{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc31a63-d4fc-4341-bf81-64f350f5010f",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://www.tensorflow.org/text/guide/bert_preprocessing_guide\n",
    "- https://stackoverflow.com/questions/58507400/how-to-use-tf-lookup-tables-with-tensorflow-2-0-keras-and-mlflow\n",
    "- https://www.tensorflow.org/text/guide/subwords_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abb9cdc-3273-4a78-998d-d8ff122de046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 09:53:58.526676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-07 09:53:58.652355: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-07 09:53:58.652376: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-07 09:53:59.478668: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-07 09:53:59.478737: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-07 09:53:59.478744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "import tensorflow_text as tft\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43b61a-2579-4b68-93ca-5e39b7cd363e",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf00233-baa9-4c48-9b28-86143a91d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 358373 entries, 0 to 358372\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       358373 non-null  object\n",
      " 1   1       358373 non-null  object\n",
      " 2   2       358373 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "path_to_file = Path().home() / \"tensorflow_datasets\" / \"anki\" / \"ita-eng\" / \"ita.txt\"\n",
    "df = pd.read_csv(path_to_file, sep=\"\\t\", header=None)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7536441b-7ad7-4273-aff8-071f3eae98e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 09:54:03.014286: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/calcifer/git/marco/learn-deep-learning/.env/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-07 09:54:03.014307: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-07 09:54:03.014324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (calcifer-Inspiron-7370): /proc/driver/nvidia/version does not exist\n",
      "2023-01-07 09:54:03.014550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(df.head(1000)[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bebdcd-9d87-4a2e-b119-55857a10f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: b'Hi.'\n"
     ]
    }
   ],
   "source": [
    "print(\"Example:\", ds.take(1).as_numpy_iterator().next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1faee5-df23-4e8f-8ddb-30f6e014585f",
   "metadata": {},
   "source": [
    "## Subword vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc2cc0cc-1b34-4c96-bc84-290ebc74bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "RESERVED_TOKENS = [\"[START]\", \"[END]\", \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "BERT_TOKENIZER_PARAMS = dict(\n",
    "    lower_case=True,\n",
    "    keep_whitespace=None,\n",
    "    normalization_form=None,\n",
    "    preserve_unused_token=None,\n",
    ")\n",
    "VOCAB_FILE = Path.cwd() / \"vocab.txt\"\n",
    "\n",
    "SAMPLE_SENTENCE = \"hi this is me\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401ae29-3e3c-4b6e-bc26-08428fa1bc9d",
   "metadata": {},
   "source": [
    "### Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ccd76ca-cdae-4474-87af-8b3c86f3d987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(vocab)=<class 'list'>\n",
      "Vocab samples:\n",
      "\t##t\n",
      "\tstood\n",
      "\t##or\n",
      "\tt\n",
      "\thelp\n",
      "\t!\n",
      "\tate\n",
      "\t##a\n",
      "\tstay\n",
      "\t##i\n"
     ]
    }
   ],
   "source": [
    "vocab = bert_vocab_from_dataset.bert_vocab_from_dataset(\n",
    "    dataset=ds,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    reserved_tokens=RESERVED_TOKENS,\n",
    "    bert_tokenizer_params=BERT_TOKENIZER_PARAMS,\n",
    "    learn_params=None,\n",
    ")\n",
    "print(f\"{type(vocab)=}\")\n",
    "samples_idx = np.random.randint(0, len(vocab), size=10).tolist()\n",
    "print(\n",
    "    \"Vocab samples:\", \n",
    "    *[\"\\t\"+vocab[idx] for idx in samples_idx], \n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1483feb0-0a04-43e9-aefb-f6443276fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_FILE, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7df47-f149-4715-91ad-5577f4d155f3",
   "metadata": {},
   "source": [
    "### Option 1: Create tokenizer to be used in custom module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c26df2fe-e2d9-4ecd-b110-4cf9c1e474a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "      keys=vocab,\n",
    "      key_dtype=tf.string,\n",
    "      values=tf.range(\n",
    "          tf.size(vocab, out_type=tf.int64), dtype=tf.int64),\n",
    "          value_dtype=tf.int64\n",
    "        ),\n",
    "      num_oov_buckets=1\n",
    ")\n",
    "\n",
    "tokenizer = bert_vocab_from_dataset.bert_tokenizer.BertTokenizer(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bceb828-f217-4821-842a-a9fdbdb5b775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens.shape=TensorShape([1, None, None])\n",
      "tokens=<tf.RaggedTensor [[[21, 132], [119], [22, 57], [47]]]>\n",
      "offsets_start=<tf.RaggedTensor [[[0, 1], [3], [8, 9], [11]]]>\n",
      "offsets_end=<tf.RaggedTensor [[[1, 2], [7], [9, 10], [13]]]>\n",
      "\n",
      "Output:         h      ##i     this        i      ##s       me\n",
      "Offsets:   (0, 1)   (1, 2)   (3, 7)   (8, 9)  (9, 10) (11, 13)\n"
     ]
    }
   ],
   "source": [
    "tokens, offsets_start, offsets_end = tokenizer.tokenize_with_offsets(SAMPLE_SENTENCE)\n",
    "print(f\"{tokens.shape=}\")\n",
    "print(f\"{tokens=}\")\n",
    "print(f\"{offsets_start=}\")\n",
    "print(f\"{offsets_end=}\")\n",
    "\n",
    "print()\n",
    "print(\"Output: \", *[f\"{vocab[tkn]:>8}\" for r in tokens.numpy() for word in r for tkn in word])\n",
    "# Repackage offsets for display\n",
    "offsets = tf.stack([offsets_start, offsets_end], axis=3)\n",
    "offsets = tf.squeeze(offsets, axis=0)\n",
    "print(\n",
    "    \"Offsets:\", \n",
    "    *[\n",
    "        f\"{str((s, e)):>8}\"\n",
    "        for o in offsets.numpy()  # iterate over each offset pair\n",
    "        for s, e in o             # unpack start and end offset\n",
    "        if s+e>0                  # skip empty tokens\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432e0ed-5667-4dbc-b7c4-2a0f739b2ecc",
   "metadata": {},
   "source": [
    "### Option 2: use premade layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8d79a-9158-4d06-b20a-3d70cd12522f",
   "metadata": {},
   "source": [
    "If error in tokenizer initialization, restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d93b3ee5-c4bc-4776-a76c-0e07dae322bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfm.nlp.layers.BertTokenizer(\n",
    "    vocab_file=VOCAB_FILE.as_posix(),\n",
    "    lower_case=True,\n",
    "    tokenize_with_offsets=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08b3bd98-4068-4a09-9a48-9f3e12ea10a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[[21, 132], [119], [22, 57], [47]]]>,\n",
       " <tf.RaggedTensor [[[0, 1], [3], [8, 9], [11]]]>,\n",
       " <tf.RaggedTensor [[[1, 2], [7], [9, 10], [13]]]>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([tf.constant(SAMPLE_SENTENCE)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
